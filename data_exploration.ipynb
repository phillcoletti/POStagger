{
 "metadata": {
  "name": "data_exploration"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import brown\n",
      "from pprint import pprint\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import math\n",
      "import json"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eps = 10e-50"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Extract features from all sentences\n",
      "feature_list = []\n",
      "i = 0\n",
      "for sentence in brown.tagged_sents(simplify_tags=True):\n",
      "    if len(sentence) >= 2:\n",
      "        (w1,t1) = sentence.pop(0)\n",
      "        (w2,t2) = sentence.pop(0)\n",
      "        tags = ['start', 'start', 'start', t1, t2]\n",
      "        words = [None, None, None, w1, w2]\n",
      "        sentence.append((None, 'end'))\n",
      "        sentence.append((None, 'end'))\n",
      "        for (w,t) in sentence:\n",
      "            #Shift all words and tags down one slot\n",
      "            tags.pop(0)\n",
      "            tags.append(t)\n",
      "            words.pop(0)\n",
      "            words.append(w)\n",
      "            extra_features = [#(words[1], words[2]), (words[2], words[3]),    #lexical\n",
      "                              #(words[2], tags[1]), (words[2], tags[3]),      #lexical-word combo\n",
      "                              tags[0] + '_' + tags[1], tags[1] + '_' + tags[3], tags[3] + '_' + tags[4]]  #tag sequence\n",
      "            features = [i] + tags + words + extra_features\n",
      "            feature_list.append(features)\n",
      "        i = i + 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_names = ['sentence_num', 't_i-2', 't_i-1', 't_i', 't_i+1', 't_i+2', 'w_i-2', 'w_i-1', 'w_i', 'w_i+1', 'w_i+2', \n",
      "                #'w_i-1,i', 'w_i,i+1', 'w_i,t_i-1', 'w_i,t_i+1', \n",
      "                't_i-2,i-1', 't_i-1,i+1', 't_i+1,i+2']\n",
      "word_features = pd.DataFrame(feature_list, columns=feature_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logprob_features = {}\n",
      "if 't_i' in feature_names:\n",
      "    feature_names.remove('t_i')\n",
      "for feature_name in feature_names:\n",
      "    frequency = pd.crosstab(rows=word_features['t_i'], cols=word_features[feature_name]) #calculate frequency table for feature vs. t_i\n",
      "    feature_sum = frequency.sum().astype(float)  #sum across all possible t_i's for a given feature value\n",
      "    supported = feature_sum[(feature_sum >= 2)].index\n",
      "#    unsupported = feature_sum[(feature_sum < 2)].index  #Support Cutoff\n",
      "    p = frequency / feature_sum    #normalize to become conditional probability of t_i (given feature value)\n",
      "    p = p[supported]        #ignore columns below support cutoff\n",
      "    p[p < eps] = eps\n",
      "    logp = np.log(p)\n",
      "#    p[unsupported] = None  #don't count those feature values that are below cutoff\n",
      "    logprob_features[feature_name] = logp.to_dict()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#word_features.to_csv('brown_corpus_word_features.csv')\n",
      "#with open('brown_corpus_prob_features.json', 'w') as outfile:\n",
      "#    json.dump( prob_features, outfile )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prior_count = word_features['t_i'].value_counts()\n",
      "total_count = prior_count.sum()\n",
      "prior_prob = prior_count / float(total_count)\n",
      "prior_prob[prior_prob < eps] = eps\n",
      "logprior = np.log(prior_prob)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_features['t_i'].unique()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "array(['DET', 'NP', 'N', 'ADJ', 'VD', 'P', '``', \"''\", 'CNJ', '.', 'ADV',\n",
        "       ',', 'WH', 'V', 'VBZ', 'VN', 'TO', 'PRO', 'VG', 'NUM', 'MOD', '*',\n",
        "       'EX', ':', '(', ')', '', \"'\", 'FW', 'UH', 'VB+PPO', 'NIL', 'VB+IN',\n",
        "       'VBG+TO', 'VBN+TO', 'VB+RP', 'VB+JJ', 'VB+VB', 'VB+TO', 'VB+AT'], dtype=object)"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}